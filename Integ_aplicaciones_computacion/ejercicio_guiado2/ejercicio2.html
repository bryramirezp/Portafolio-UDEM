<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ejercicio 2: Chat con IA Local usando Docker y Ollama</title>
    <link rel="stylesheet" href="../style.css"> 
    <style>
        .article-content {
            background-color: #ffffff;
            padding: 2.5rem;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            max-width: 900px;
            margin: 2rem auto;
        }
        .article-content h2 {
            color: #3498db;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 0.5rem;
            margin-top: 2rem;
        }
        .article-content h3 {
            color: #2c3e50;
            margin-top: 1.5rem;
        }
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
        }
        .article-image {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin-top: 1rem;
            border: 1px solid #ddd;
        }
        .image-caption {
            text-align: center;
            font-style: italic;
            color: #777;
            margin-bottom: 2rem;
        }
    </style>
</head>
<body>

    <header class="header">
        <h1>Portafolio de Tareas</h1>
        <p>Integración de Aplicaciones Computacionales</p>
    </header>

    <main class="content">
        <article class="article-content">
            <h1>Ejercicio 2: Chat con IA Local usando Docker y Ollama 🤖</h1>
            <p>Este ejercicio demuestra cómo crear y orquestar un sistema de dos contenedores Docker para desplegar una aplicación de chat que interactúa con un modelo de lenguaje grande (LLM) local. La arquitectura se compone de:</p>
            <ul>
                <li>Un contenedor con **Ollama** sirviendo el modelo **Deepseek-Coder**.</li>
                <li>Un contenedor con un servidor web **Apache** que aloja la interfaz de chat (HTML, CSS, JS).</li>
                <li>Una red de Docker para permitir la comunicación entre ambos contenedores.</li>
            </ul>

            <h2>Paso 1: Preparación del Entorno (Ollama y el Modelo)</h2>
            <p>El primer paso es levantar el servicio de Ollama en un contenedor y descargar el modelo que utilizaremos. Esto aísla el entorno de IA y facilita su gestión.</p>
            
            <h3>1.1. Correr el contenedor de Ollama</h3>
            <p>Este comando descarga la imagen oficial de Ollama y la ejecuta, exponiendo el puerto 11434 para que nuestra aplicación pueda comunicarse con la API.</p>
            <pre><code>docker run -d -p 11434:11434 --name ollama ollama/ollama</code></pre>

            <h3>1.2. Descargar el modelo Deepseek-Coder</h3>
            <p>Una vez que el contenedor `ollama` está en ejecución, usamos `docker exec` para ejecutar un comando dentro de él. En este caso, le pedimos a Ollama que descargue (`pull`) el modelo `deepseek-coder`.</p>
            <pre><code>docker exec -it ollama ollama pull deepseek-coder</code></pre>

            <h2>Paso 2: Creación de la Interfaz Web</h2>
            <p>La interfaz es una aplicación web simple que consta de tres archivos principales: `index.html` (estructura), `style.css` (diseño) y `app.js` (lógica).</p>
            <p>El archivo clave es <strong>app.js</strong>, que se encarga de enviar las solicitudes del usuario a la API de Ollama y mostrar la respuesta en el chat. La conexión se define con la siguiente URL, apuntando al nombre del contenedor de Ollama, que actuará como un hostname dentro de la red de Docker.</p>
            <pre><code>// Dentro de app.js
const OLLAMA_API_URL = 'http://ollama:11434/api/generate';</code></pre>

            <h2>Paso 3: Containerizar la Aplicación Web</h2>
            <p>Para empaquetar nuestra aplicación web, utilizamos un `Dockerfile` que define una imagen basada en el servidor web Apache (`httpd`).</p>
            
            <h3>3.1. Dockerfile</h3>
            <p>Este archivo de configuración le indica a Docker que use la imagen de Apache como base y copie nuestros archivos (`index.html`, `style.css`, `app.js`) al directorio donde Apache sirve el contenido web.</p>
            <pre><code># Usa la imagen oficial de Apache httpd
FROM httpd:latest

# Copia los archivos de tu página web al directorio por defecto de Apache
COPY ./index.html /usr/local/apache2/htdocs/
COPY ./style.css /usr/local/apache2/htdocs/
COPY ./app.js /usr/local/apache2/htdocs/</code></pre>
            
            <h3>3.2. Construir la imagen</h3>
            <p>Con el `Dockerfile` en nuestro directorio, ejecutamos el comando `docker build` para crear la imagen, a la que llamamos `mi-apache-chat`.</p>
            <pre><code>docker build -t mi-apache-chat .</code></pre>
            
            <img src="Integ_aplicaciones_computacion/ejercicio_guiado2/docker_build_vscode.png" alt="Construcción de la imagen Docker en Visual Studio Code" class="article-image">
            <p class="image-caption">Captura del proceso de `docker build` en la terminal de VS Code.</p>


            <h2>Paso 4: Red y Despliegue de Contenedores</h2>
            <p>Para que el contenedor de Apache pueda comunicarse con el de Ollama, ambos deben estar en la misma red de Docker. Si no lo hacemos, el `fetch` en JavaScript a `http://ollama:11434` fallaría.</p>

            <h3>4.1. Crear una red de Docker</h3>
            <p>Creamos una nueva red tipo "bridge" llamada `mi-red`.</p>
            <pre><code>docker network create mi-red</code></pre>

            <h3>4.2. Conectar los contenedores a la red</h3>
            <p>Conectamos tanto el contenedor `ollama` (ya en ejecución) como el futuro contenedor de Apache a esta red.</p>
            <pre><code>docker network connect mi-red ollama
# (El contenedor de apache se conectará al crearlo)</code></pre>
            
            <img src="Integ_aplicaciones_computacion/ejercicio_guiado2/docker_network_commands.png" alt="Comandos para gestionar la red de Docker" class="article-image">
            <p class="image-caption">Ejecución de los comandos `docker network` en la terminal.</p>


            <h3>4.3. Correr el contenedor de la aplicación web</h3>
            <p>Finalmente, ejecutamos el contenedor de nuestra aplicación web, conectándolo a `mi-red`, exponiendo el puerto 8080 y nombrándolo `mi-apache-chat`.</p>
            <pre><code>docker run -d -p 8080:80 --name mi-apache-chat --network mi-red mi-apache-chat</code></pre>

            <h2>Paso 5: Resultado Final</h2>
            <p>Al acceder a `http://localhost:8080` en el navegador, la interfaz de chat se carga y es completamente funcional. Las solicitudes del usuario viajan desde el frontend en el contenedor de Apache hasta el backend de IA en el contenedor de Ollama a través de la red interna de Docker.</p>

            <img src="Integ_aplicaciones_computacion/ejercicio_guiado2/chat_resultado_final.png" alt="Aplicación de chat funcionando" class="article-image">
            <p class="image-caption">La aplicación final interactuando con el modelo Deepseek-Coder.</p>
            
            <a href="../index.html" class="back-link" style="display: inline-block; margin-top: 2rem; color: #3498db; text-decoration: none;">&larr; Volver al Portafolio</a>
        </article>
    </main>

    <footer class="footer">
        <p>© 2025 Bryan Andrés Ramírez Palacios </p>
    </footer>

</body>
</html>
