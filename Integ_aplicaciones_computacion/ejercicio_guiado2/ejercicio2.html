<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ejercicio 2: Chat con IA Local usando Docker y Ollama</title>
    <link rel="stylesheet" href="../style.css"> 
    <style>
        .article-content {
            background-color: #ffffff;
            padding: 2.5rem;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            max-width: 900px;
            margin: 2rem auto;
        }
        .article-content h2 {
            color: #3498db;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 0.5rem;
            margin-top: 2rem;
        }
        .article-content h3 {
            color: #2c3e50;
            margin-top: 1.5rem;
        }
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
        }
        .article-image {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin-top: 1rem;
            border: 1px solid #ddd;
        }
        .image-caption {
            text-align: center;
            font-style: italic;
            color: #777;
            margin-bottom: 2rem;
        }
    </style>
</head>
<body>

    <header class="header">
        <h1>Portafolio de Tareas</h1>
        <p>Integraci贸n de Aplicaciones Computacionales</p>
    </header>

    <main class="content">
        <article class="article-content">
            <h1>Ejercicio 2: Chat con IA Local usando Docker y Ollama </h1>
            <p>Este ejercicio demuestra c贸mo crear y orquestar un sistema de dos contenedores Docker para desplegar una aplicaci贸n de chat que interact煤a con un modelo de lenguaje grande (LLM) local. La arquitectura se compone de:</p>
            <ul>
                <li>Un contenedor con **Ollama** sirviendo el modelo **Deepseek-Coder**.</li>
                <li>Un contenedor con un servidor web **Apache** que aloja la interfaz de chat (HTML, CSS, JS).</li>
                <li>Una red de Docker para permitir la comunicaci贸n entre ambos contenedores.</li>
            </ul>

            <h2>Paso 1: Preparaci贸n del Entorno (Ollama y el Modelo)</h2>
            <p>El primer paso es levantar el servicio de Ollama en un contenedor y descargar el modelo que utilizaremos. Esto a铆sla el entorno de IA y facilita su gesti贸n.</p>
            
            <h3>1.1. Correr el contenedor de Ollama</h3>
            <p>Este comando descarga la imagen oficial de Ollama y la ejecuta, exponiendo el puerto 11434 para que nuestra aplicaci贸n pueda comunicarse con la API.</p>
            <pre><code>docker run -d -p 11434:11434 --name ollama ollama/ollama</code></pre>

            <h3>1.2. Descargar el modelo Deepseek-Coder</h3>
            <p>Una vez que el contenedor `ollama` est谩 en ejecuci贸n, usamos `docker exec` para ejecutar un comando dentro de 茅l. En este caso, le pedimos a Ollama que descargue (`pull`) el modelo `deepseek-coder`.</p>
            <pre><code>docker exec -it ollama ollama pull deepseek-coder</code></pre>

            <h2>Paso 2: Creaci贸n de la Interfaz Web</h2>
            <p>La interfaz es una aplicaci贸n web simple que consta de tres archivos principales: `index.html` (estructura), `style.css` (dise帽o) y `app.js` (l贸gica).</p>
            <p>El archivo clave es <strong>app.js</strong>, que se encarga de enviar las solicitudes del usuario a la API de Ollama y mostrar la respuesta en el chat. La conexi贸n se define con la siguiente URL, apuntando al nombre del contenedor de Ollama, que actuar谩 como un hostname dentro de la red de Docker.</p>
            <pre><code>// Dentro de app.js
const OLLAMA_API_URL = 'http://ollama:11434/api/generate';</code></pre>

            <h2>Paso 3: Containerizar la Aplicaci贸n Web</h2>
            <p>Para empaquetar nuestra aplicaci贸n web, utilizamos un `Dockerfile` que define una imagen basada en el servidor web Apache (`httpd`).</p>
            
            <h3>3.1. Dockerfile</h3>
            <p>Este archivo de configuraci贸n le indica a Docker que use la imagen de Apache como base y copie nuestros archivos (`index.html`, `style.css`, `app.js`) al directorio donde Apache sirve el contenido web.</p>
            <pre><code># Usa la imagen oficial de Apache httpd
FROM httpd:latest

# Copia los archivos de tu p谩gina web al directorio por defecto de Apache
COPY ./index.html /usr/local/apache2/htdocs/
COPY ./style.css /usr/local/apache2/htdocs/
COPY ./app.js /usr/local/apache2/htdocs/</code></pre>
            
            <h3>3.2. Construir la imagen</h3>
            <p>Con el `Dockerfile` en nuestro directorio, ejecutamos el comando `docker build` para crear la imagen, a la que llamamos `mi-apache-chat`.</p>
            <pre><code>docker build -t mi-apache-chat .</code></pre>
            
            <img src="Integ_aplicaciones_computacion/ejercicio_guiado2/docker_build_vscode.png" alt="Construcci贸n de la imagen Docker en Visual Studio Code" class="article-image">
            <p class="image-caption">Captura del proceso de `docker build` en la terminal de VS Code.</p>


            <h2>Paso 4: Red y Despliegue de Contenedores</h2>
            <p>Para que el contenedor de Apache pueda comunicarse con el de Ollama, ambos deben estar en la misma red de Docker. Si no lo hacemos, el `fetch` en JavaScript a `http://ollama:11434` fallar铆a.</p>

            <h3>4.1. Crear una red de Docker</h3>
            <p>Creamos una nueva red tipo "bridge" llamada `mi-red`.</p>
            <pre><code>docker network create mi-red</code></pre>

            <h3>4.2. Conectar los contenedores a la red</h3>
            <p>Conectamos tanto el contenedor `ollama` (ya en ejecuci贸n) como el futuro contenedor de Apache a esta red.</p>
            <pre><code>docker network connect mi-red ollama
# (El contenedor de apache se conectar谩 al crearlo)</code></pre>
            
            <img src="Integ_aplicaciones_computacion/ejercicio_guiado2/docker_network_commands.png" alt="Comandos para gestionar la red de Docker" class="article-image">
            <p class="image-caption">Ejecuci贸n de los comandos `docker network` en la terminal.</p>


            <h3>4.3. Correr el contenedor de la aplicaci贸n web</h3>
            <p>Finalmente, ejecutamos el contenedor de nuestra aplicaci贸n web, conect谩ndolo a `mi-red`, exponiendo el puerto 8080 y nombr谩ndolo `mi-apache-chat`.</p>
            <pre><code>docker run -d -p 8080:80 --name mi-apache-chat --network mi-red mi-apache-chat</code></pre>

            <h2>Paso 5: Resultado Final</h2>
            <p>Al acceder a `http://localhost:8080` en el navegador, la interfaz de chat se carga y es completamente funcional. Las solicitudes del usuario viajan desde el frontend en el contenedor de Apache hasta el backend de IA en el contenedor de Ollama a trav茅s de la red interna de Docker.</p>

            <img src="Integ_aplicaciones_computacion/ejercicio_guiado2/chat_resultado_final.png" alt="Aplicaci贸n de chat funcionando" class="article-image">
            <p class="image-caption">La aplicaci贸n final interactuando con el modelo Deepseek-Coder.</p>
            
            <a href="../index.html" class="back-link" style="display: inline-block; margin-top: 2rem; color: #3498db; text-decoration: none;">&larr; Volver al Portafolio</a>
        </article>
    </main>

    <footer class="footer">
        <p>漏 2025 Bryan Andr茅s Ram铆rez Palacios </p>
    </footer>

</body>
</html>
